{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d114fd35",
   "metadata": {},
   "source": [
    "# Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff14d8",
   "metadata": {},
   "source": [
    "# S.2 SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce77ccf",
   "metadata": {},
   "source": [
    "## 2.1 경로추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465e6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#full-version\n",
    "os.environ[\"SPARK_HOME\"]=\"C:\\\\spark\\\\spark-3.1.2-bin-hadoop3.2\"\n",
    "os.environ[\"PYLIB\"]=\"C:\\\\spark\\\\spark-3.1.2-bin-hadoop3.2\\\\python\\\\lib\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Users\\\\SW\\\\anaconda3\\\\python.exe\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Users\\\\SW\\\\anaconda3\\\\python.exe\"\n",
    "# os.environ[\"JAVA_HOME\"]=\"C:\\\\Program Files\\\\Java\\\\jdk-11.0.11\\\\bin\"\n",
    "\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],\"py4j-0.10.9-src.zip\"))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],\"pyspark.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582b6d1",
   "metadata": {},
   "source": [
    "## 2.2 세션생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655ddc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"ds3_\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e17d4",
   "metadata": {},
   "source": [
    "# S.3 데이터구조\n",
    "\n",
    "* RDD\n",
    "* DataFrame\n",
    "* Dataset\n",
    "\n",
    "모두 immutable(생성 후 원본 수정 불가)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a65e5f",
   "metadata": {},
   "source": [
    "# S.5 RDD 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d3bb",
   "metadata": {},
   "source": [
    "## 1. List에서 RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eadb32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "# RDD 생성\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "# RDD 실행\n",
    "print(myRdd1.take(3))\n",
    "spark.sparkContext.parallelize([0, 2, 3, 4, 6], 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa76cf",
   "metadata": {},
   "source": [
    "- glom() : 각 partition에 있는 요소를 묶어서 RDD 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653eb655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2], [3, 4, 6]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([0, 2, 3, 4, 6], 2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc29842",
   "metadata": {},
   "source": [
    "## 2. 파일에서 RDD 생성하기\n",
    "\n",
    "data/ds_spark_wiki.txt\n",
    "```\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da35a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wikipedia'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "myRdd2 = spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_wiki.txt'))\n",
    "\n",
    "#first()함수는 첫 번째 데이터 조회\n",
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e11ec",
   "metadata": {},
   "source": [
    "## 3. csv에서 RDD 생성하기\n",
    "./data/ds_spark_2cols.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1df264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_2cols.csv'))\n",
    "\n",
    "myList=myRdd4.take(5)\n",
    "print(type(myList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63369c32",
   "metadata": {},
   "source": [
    "# 문제: 파일에서 RDD 생성\n",
    "1) 경기도 의정부시 인구현황 (파일명: 경기도 의정부시_인구현황_20210910.csv) https://www.data.go.kr/data/15009613/fileData.do\n",
    "\n",
    "2) 제주특별자치도 서귀포시 내 연도별 65세이상 인구수 및 고령화비율, 노령화지수 현황 (파일명: 제주특별자치도 서귀포시_고령화비율및노령화지수현황_20210831.csv) https://www.data.go.kr/data/15051545/fileData.do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7decb",
   "metadata": {},
   "source": [
    "## 1. RDD로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d62d74b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['�������,�α���(��),�α���(��),�α���(��),������(��),������(��),������(��),����,�����,������α�,���������,�����μ���,�μ���ȭ��ȣ,�����ͱ�������', '������1��,32292,16538,15754,6.97,3.57,3.4,104.98,19998,1.61,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10', '������2��,31380,15608,15772,6.77,3.37,3.4,98.96,16410,1.91,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10', 'ȣ��1��,36124,17595,18529,7.8,3.8,4,94.96,15653,2.31,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10', 'ȣ��2��,34957,16923,18034,7.54,3.65,3.89,93.84,13683,2.55,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10'] \n",
      "\n",
      "������,�������� �α���,65���̻� �α��� ,14������ �α���,���ȭ����,���ȭ����,�����ͱ�������\n",
      "2012,154057,25826,22861,16.76,112.97,2021-08-31\n",
      "2013,155641,26936,22393,17.31,120.29,2021-08-31\n",
      "2014,158512,27877,22058,17.59,126.38,2021-08-31\n",
      "2015,164519,28979,22362,17.61,129.59,2021-08-31\n"
     ]
    }
   ],
   "source": [
    "popRdd =spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','경기도 의정부시_인구현황_20210910.csv'))\n",
    "agedRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"제주특별자치도 서귀포시_고령화비율및노령화지수현황_20210831.csv\"), use_unicode=True)\n",
    "\n",
    "print(popRdd.take(5),'\\n')\n",
    "\n",
    "for i in agedRdd.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a692b3e1",
   "metadata": {},
   "source": [
    "## 2. binaryFiles\n",
    "이진 파일을 읽는 함수, 이진 파일로 읽어 한글을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1002dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['행정기관,인구수(계),인구수(남),인구수(여),구성비(계),구성비(남),구성비(여),성비,세대수,세대당인구,관리기관명,관리부서명,부서전화번호,데이터기준일자\\r\\n의정부1동,32292,16538,15754,6.97,3.57,3.4,104.98,19998,1.61,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n의정부2동,31380,15608,15772,6.77,3.37,3.4,98.96,16410,1.91,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n호원1동,36124,17595,18529,7.8,3.8,4,94.96,15653,2.31,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n호원2동,34957,16923,18034,7.54,3.65,3.89,93.84,13683,2.55,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n장암동,20314,9714,10600,4.38,2.1,2.29,91.64,8604,2.36,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n신곡1동,43159,21205,21954,9.31,4.58,4.74,96.59,17990,2.4,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n신곡2동,47852,23232,24620,10.33,5.01,5.31,94.36,19218,2.49,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n송산1동,42817,21276,21541,9.24,4.59,4.65,98.77,18811,2.28,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n송산2동,33565,16601,16964,7.24,3.58,3.66,97.86,13216,2.54,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n송산3동,46892,22772,24120,10.12,4.91,5.21,94.41,17926,2.62,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n자금동,27087,13270,13817,5.85,2.86,2.98,96.04,11868,2.28,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n가능동,25990,12974,13016,5.61,2.8,2.81,99.68,12492,2.08,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n흥선동,19176,9769,9407,4.14,2.11,2.03,103.85,9380,2.04,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n녹양동,21768,10872,10896,4.7,2.35,2.35,99.78,9556,2.28,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popRddBin = spark.sparkContext.binaryFiles(os.path.join('data','경기도 의정부시_인구현황_20210910.csv'))\n",
    "_my = popRddBin.map(lambda x : x[1].decode('euc-kr'))\n",
    "_my.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6674f9",
   "metadata": {},
   "source": [
    "# 3. spark-submit 실행\n",
    "py파일로 저장하고 spark-submit 명령으로 실행가능\n",
    "\n",
    "src/ds3_popCsvRead.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c38f724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds3_popCsvRead.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds3_popCsvRead.py\n",
    "#python3\n",
    "#coding: UTF-8\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    print(\"-----RESULT------\")\n",
    "    popDf = spark\\\n",
    "        .read.option(\"charset\",\"euc-kr\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .csv(os.path.join(\"data\",\"경기도 의정부시_인구현황_20210910.csv\"))\n",
    "    popDf.show(5)\n",
    "    agedDf = spark\\\n",
    "        .read.option(\"charset\",\"euc-kr\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .csv(os.path.join(\"data\",\"제주특별자치도 서귀포시_고령화비율및노령화지수현황_20210831.csv\"))\n",
    "    agedDf.show(5)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Users\\\\SW\\\\anaconda3\\\\python.exe\"\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Users\\\\SW\\\\anaconda3\\\\python.exe\"\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark=pyspark.sql.SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2427747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----RESULT------\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "| 행정기관|인구수(계)|인구수(남)|인구수(여)|구성비(계)|구성비(남)|구성비(여)|  성비|세대수|세대당인구|관리기관명|관리부서명|부서전화번호|데이터기준일자|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "|의정부1동|     32292|     16538|     15754|      6.97|      3.57|       3.4|104.98| 19998|      1.61|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|의정부2동|     31380|     15608|     15772|      6.77|      3.37|       3.4| 98.96| 16410|      1.91|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|  호원1동|     36124|     17595|     18529|       7.8|       3.8|         4| 94.96| 15653|      2.31|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|  호원2동|     34957|     16923|     18034|      7.54|      3.65|      3.89| 93.84| 13683|      2.55|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|   장암동|     20314|      9714|     10600|      4.38|       2.1|      2.29| 91.64|  8604|      2.36|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|연도별|서귀포시 인구수|65세이상 인구수 |14세이하 인구수|고령화비율|노령화지수|데이터기준일자|\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|  2012|         154057|           25826|          22861|     16.76|    112.97|    2021-08-31|\n",
      "|  2013|         155641|           26936|          22393|     17.31|    120.29|    2021-08-31|\n",
      "|  2014|         158512|           27877|          22058|     17.59|    126.38|    2021-08-31|\n",
      "|  2015|         164519|           28979|          22362|     17.61|    129.59|    2021-08-31|\n",
      "|  2016|         170932|           30030|          23044|     17.57|    130.32|    2021-08-31|\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/spark/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/10/22 10:25:10 INFO SparkContext: Running Spark version 3.1.2\n",
      "21/10/22 10:25:10 INFO ResourceUtils: ==============================================================\n",
      "21/10/22 10:25:10 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/10/22 10:25:10 INFO ResourceUtils: ==============================================================\n",
      "21/10/22 10:25:10 INFO SparkContext: Submitted application: myApp\n",
      "21/10/22 10:25:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/10/22 10:25:10 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/10/22 10:25:10 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/10/22 10:25:10 INFO SecurityManager: Changing view acls to: SW중심대학사업단,SW?????л?¾÷\n",
      "21/10/22 10:25:10 INFO SecurityManager: Changing modify acls to: SW중심대학사업단,SW?????л?¾÷\n",
      "21/10/22 10:25:10 INFO SecurityManager: Changing view acls groups to: \n",
      "21/10/22 10:25:10 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/10/22 10:25:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SW중심대학사업단, SW?????л?¾÷); groups with view permissions: Set(); users  with modify permissions: Set(SW중심대학사업단, SW?????л?¾÷); groups with modify permissions: Set()\n",
      "21/10/22 10:25:11 INFO Utils: Successfully started service 'sparkDriver' on port 51409.\n",
      "21/10/22 10:25:11 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/10/22 10:25:11 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/10/22 10:25:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/10/22 10:25:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/10/22 10:25:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/10/22 10:25:11 INFO DiskBlockManager: Created local directory at C:\\Users\\SW\\AppData\\Local\\Temp\\blockmgr-9e4db256-c316-4a49-afd8-575c61963b18\n",
      "21/10/22 10:25:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "21/10/22 10:25:11 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/10/22 10:25:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/22 10:25:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "21/10/22 10:25:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-PQ8LIGD:4041\n",
      "21/10/22 10:25:12 INFO Executor: Starting executor ID driver on host DESKTOP-PQ8LIGD\n",
      "21/10/22 10:25:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51424.\n",
      "21/10/22 10:25:12 INFO NettyBlockTransferService: Server created on DESKTOP-PQ8LIGD:51424\n",
      "21/10/22 10:25:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/10/22 10:25:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-PQ8LIGD, 51424, None)\n",
      "21/10/22 10:25:12 INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-PQ8LIGD:51424 with 434.4 MiB RAM, BlockManagerId(driver, DESKTOP-PQ8LIGD, 51424, None)\n",
      "21/10/22 10:25:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-PQ8LIGD, 51424, None)\n",
      "21/10/22 10:25:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-PQ8LIGD, 51424, None)\n",
      "21/10/22 10:25:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/SW/Code/yhm/spark-warehouse/').\n",
      "21/10/22 10:25:13 INFO SharedState: Warehouse path is 'file:/C:/Users/SW/Code/yhm/spark-warehouse/'.\n",
      "21/10/22 10:25:15 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.\n",
      "21/10/22 10:25:15 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
      "21/10/22 10:25:18 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/10/22 10:25:18 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/10/22 10:25:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "21/10/22 10:25:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.9 KiB, free 434.2 MiB)\n",
      "21/10/22 10:25:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)\n",
      "21/10/22 10:25:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 27.6 KiB, free: 434.4 MiB)\n",
      "21/10/22 10:25:18 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195881 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/10/22 10:25:19 INFO CodeGenerator: Code generated in 315.532799 ms\n",
      "21/10/22 10:25:19 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:19 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/10/22 10:25:19 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "21/10/22 10:25:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/10/22 10:25:19 INFO DAGScheduler: Missing parents: List()\n",
      "21/10/22 10:25:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/10/22 10:25:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.3 KiB, free 434.2 MiB)\n",
      "21/10/22 10:25:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.2 MiB)\n",
      "21/10/22 10:25:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "21/10/22 10:25:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/22 10:25:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/22 10:25:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/10/22 10:25:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-PQ8LIGD, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes) taskResourceAssignments Map()\n",
      "21/10/22 10:25:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/10/22 10:25:20 INFO FileScanRDD: Reading File path: file:///C:/Users/SW/Code/yhm/data/경기도%20의정부시_인구현황_20210910.csv, range: 0-1577, partition values: [empty row]\n",
      "21/10/22 10:25:20 INFO CodeGenerator: Code generated in 26.8301 ms\n",
      "21/10/22 10:25:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1770 bytes result sent to driver\n",
      "21/10/22 10:25:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 485 ms on DESKTOP-PQ8LIGD (executor driver) (1/1)\n",
      "21/10/22 10:25:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/10/22 10:25:20 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.807 s\n",
      "21/10/22 10:25:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/10/22 10:25:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "21/10/22 10:25:20 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.861964 s\n",
      "21/10/22 10:25:20 INFO CodeGenerator: Code generated in 23.928101 ms\n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Output Data Schema: struct<행정기관: string, 인구수(계): string, 인구수(남): string, 인구수(여): string, 구성비(계): string ... 12 more fields>\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.8 KiB, free 434.0 MiB)\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)\n",
      "21/10/22 10:25:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 27.6 KiB, free: 434.3 MiB)\n",
      "21/10/22 10:25:21 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195881 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/10/22 10:25:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Missing parents: List()\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.4 KiB, free 434.0 MiB)\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.0 MiB)\n",
      "21/10/22 10:25:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "21/10/22 10:25:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "21/10/22 10:25:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-PQ8LIGD, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes) taskResourceAssignments Map()\n",
      "21/10/22 10:25:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "21/10/22 10:25:21 INFO FileScanRDD: Reading File path: file:///C:/Users/SW/Code/yhm/data/경기도%20의정부시_인구현황_20210910.csv, range: 0-1577, partition values: [empty row]\n",
      "21/10/22 10:25:21 INFO CodeGenerator: Code generated in 29.765399 ms\n",
      "21/10/22 10:25:21 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2003 bytes result sent to driver\n",
      "21/10/22 10:25:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 92 ms on DESKTOP-PQ8LIGD (executor driver) (1/1)\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/10/22 10:25:21 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.108 s\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.111860 s\n",
      "21/10/22 10:25:21 INFO CodeGenerator: Code generated in 32.4465 ms\n",
      "21/10/22 10:25:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "21/10/22 10:25:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.9 KiB, free 433.8 MiB)\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.8 MiB)\n",
      "21/10/22 10:25:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 27.6 KiB, free: 434.3 MiB)\n",
      "21/10/22 10:25:21 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194887 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/10/22 10:25:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Missing parents: List()\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[20] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.3 KiB, free 433.8 MiB)\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 433.8 MiB)\n",
      "21/10/22 10:25:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 6.6 KiB, free: 434.3 MiB)\n",
      "21/10/22 10:25:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[20] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "21/10/22 10:25:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-PQ8LIGD, executor driver, partition 0, PROCESS_LOCAL, 4948 bytes) taskResourceAssignments Map()\n",
      "21/10/22 10:25:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "21/10/22 10:25:21 INFO FileScanRDD: Reading File path: file:///C:/Users/SW/Code/yhm/data/제주특별자치도%20서귀포시_고령화비율및노령화지수현황_20210831.csv, range: 0-583, partition values: [empty row]\n",
      "21/10/22 10:25:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1652 bytes result sent to driver\n",
      "21/10/22 10:25:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 18 ms on DESKTOP-PQ8LIGD (executor driver) (1/1)\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "21/10/22 10:25:21 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.030 s\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.034062 s\n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/10/22 10:25:21 INFO FileSourceStrategy: Output Data Schema: struct<연도별: string, 서귀포시 인구수: string, 65세이상 인구수 : string, 14세이하 인구수: string, 고령화비율: string ... 5 more fields>\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 174.8 KiB, free 433.6 MiB)\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.6 MiB)\n",
      "21/10/22 10:25:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 27.6 KiB, free: 434.3 MiB)\n",
      "21/10/22 10:25:21 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194887 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/10/22 10:25:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Missing parents: List()\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.7 KiB, free 433.5 MiB)\n",
      "21/10/22 10:25:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 433.5 MiB)\n",
      "21/10/22 10:25:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on DESKTOP-PQ8LIGD:51424 (size: 5.3 KiB, free: 434.3 MiB)\n",
      "21/10/22 10:25:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "21/10/22 10:25:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (DESKTOP-PQ8LIGD, executor driver, partition 0, PROCESS_LOCAL, 4948 bytes) taskResourceAssignments Map()\n",
      "21/10/22 10:25:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "21/10/22 10:25:21 INFO FileScanRDD: Reading File path: file:///C:/Users/SW/Code/yhm/data/제주특별자치도%20서귀포시_고령화비율및노령화지수현황_20210831.csv, range: 0-583, partition values: [empty row]\n",
      "21/10/22 10:25:21 INFO CodeGenerator: Code generated in 21.7339 ms\n",
      "21/10/22 10:25:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1647 bytes result sent to driver\n",
      "21/10/22 10:25:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 48 ms on DESKTOP-PQ8LIGD (executor driver) (1/1)\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "21/10/22 10:25:21 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.059 s\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/10/22 10:25:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "21/10/22 10:25:21 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.062727 s\n",
      "21/10/22 10:25:21 INFO CodeGenerator: Code generated in 22.2075 ms\n",
      "21/10/22 10:25:21 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-PQ8LIGD:4041\n",
      "21/10/22 10:25:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/10/22 10:25:21 INFO MemoryStore: MemoryStore cleared\n",
      "21/10/22 10:25:21 INFO BlockManager: BlockManager stopped\n",
      "21/10/22 10:25:21 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/10/22 10:25:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/10/22 10:25:21 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/10/22 10:25:21 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/10/22 10:25:21 INFO ShutdownHookManager: Deleting directory C:\\Users\\SW\\AppData\\Local\\Temp\\spark-17c9548b-0e51-42f2-b601-5acaf3ba789a\\pyspark-9404bac5-9dc6-4d24-9567-590a46f7fe01\n",
      "21/10/22 10:25:21 INFO ShutdownHookManager: Deleting directory C:\\Users\\SW\\AppData\\Local\\Temp\\spark-7a347d1e-0f43-4ff0-a570-3befaf9d9f4e\n",
      "21/10/22 10:25:21 INFO ShutdownHookManager: Deleting directory C:\\Users\\SW\\AppData\\Local\\Temp\\spark-17c9548b-0e51-42f2-b601-5acaf3ba789a\n"
     ]
    }
   ],
   "source": [
    "#경로 모음 실행하면 경로를 못찾는 오류\n",
    "!spark-submit src/ds3_popCsvRead.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c998c",
   "metadata": {},
   "source": [
    "# S.6 RDD API\n",
    "변환 tramsformation\n",
    "```\n",
    "함수\t설명\t예제\n",
    "map(fn)\t요소별로 fn을 적용해서 결과 RDD 돌려줌\t.map(lambda x: x.split(' ')\n",
    "filter(fn)\t요소별로 선별하여 fn을 적용해서 결과 RDD 돌려줌\t.filter(lambda x: \"Spark\" in x)\n",
    "flatMap(fn)\t요소별로 fn을 적용하고, flat해서 결과 RDD 돌려줌\t.flatMap(lambda x: x.split(' '))\n",
    "groupByKey()\tkey를 그룹해서 iterator를 돌려줌.\t\n",
    "```\n",
    "action\n",
    "```\n",
    "함수\t설명\t예제\n",
    "reduce(fn)\t요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌\treduce(lambda x,y:x+y)\n",
    "collect()\t모든 요소를 결과 list로 돌려줌\t\n",
    "count()\t요소의 갯수를 결과 list로 돌려줌\t\n",
    "take(n)\tcollect()는 전체이지만, n개만 돌려줌\ttake(1)\n",
    "countByKey()\tkey별 갯수를 세는 함수\tcountByKey().items()\n",
    "foreach(fn)\t각 데이터 항목에 함수fn을 적용\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bb867",
   "metadata": {},
   "source": [
    "## 1. Python함수로 먼저 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c3a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print (c2f(celsius))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfcb15",
   "metadata": {},
   "source": [
    "## 2. RDD 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7252d",
   "metadata": {},
   "source": [
    "### (1) map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93b5c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[29] at RDD at PythonRDD.scala:53\n",
      "[1, 4, 9, 16] \n",
      "\n",
      "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[35, 2], [40, 27], [12, 38], [15, 31], [21, 1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "squard = nRdd.map(lambda x: x*x)\n",
    "print(squard)\n",
    "print(squard.collect(),'\\n')\n",
    "\n",
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "print(myRdd5.take(5),'\\n')\n",
    "\n",
    "myRdd6 = myRdd5.map(lambda x: [int(i) for i in x])\n",
    "myRdd6.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c29601f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]\n",
      "['this is', 'AA line']\n",
      "['THIS', 'A']\n",
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "# 단어 분리, 단어 세기, ds_spark_wiki.txt 이용\n",
    "sentences = myRdd2.map(lambda x:x.split())\n",
    "print(sentences.count())\n",
    "print(myRdd2.map(lambda s:len(s)).collect())\n",
    "\n",
    "#교체\n",
    "myList=['this is','a line']\n",
    "_rdd=spark.sparkContext.parallelize(myList)\n",
    "wordRdd = _rdd.map(lambda x : x.split())\n",
    "repRdd = _rdd.map(lambda x:x.replace('a','AA'))\n",
    "print(repRdd.take(3))\n",
    "\n",
    "#대소문자 변환\n",
    "upperRdd = wordRdd.map(lambda x: x[0].upper())\n",
    "print(upperRdd.collect())\n",
    "\n",
    "upper2Rdd=wordRdd.map(lambda x: [i.upper() for i in x])\n",
    "print(upper2Rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa9f73",
   "metadata": {},
   "source": [
    "### (2) reduce\n",
    "반복문 없이 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "255aa203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n"
     ]
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "print(myRdd100.reduce(lambda sub, x : sub+x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e85af",
   "metadata": {},
   "source": [
    "단순 통계 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31e93b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum: 5050\n",
      "min: 1\n",
      "max: 100\n",
      "standard deviation: 28.86607004772212\n",
      "variable: 833.25\n"
     ]
    }
   ],
   "source": [
    "# 단순 통계 기능\n",
    "print('sum:',myRdd100.sum())\n",
    "print('min:',myRdd100.min())\n",
    "print('max:',myRdd100.max())\n",
    "print('standard deviation:',myRdd100.stdev())\n",
    "print('variable:',myRdd100.variance())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a3773",
   "metadata": {},
   "source": [
    "### (3) filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e1f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 단어 수 : 4 \n",
      "\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "\n",
      "Wikipedia Apache Spark open source cluster computing framework. 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. Apache Spark Apache Spark Apache Spark Apache Spark 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 Originally developed University of California, Berkeley's AMPLab, Spark codebase was later donated to Apache Software Foundation, which has maintained it since. Spark provides interface programming entire clusters with implicit data parallelism and fault-tolerance. "
     ]
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: 'Spark' in line)\n",
    "print('Spark 단어 수 :',myRdd_spark.count(),'\\n')\n",
    "\n",
    "#한글\n",
    "myRdd_unicode=myRdd2.filter(lambda line: u'스파크' in line)\n",
    "print(myRdd_unicode.first(),'\\n')\n",
    "\n",
    "#불용어 제거\n",
    "stopwords = ['is','am','are','the','for','a','an','at']\n",
    "myRdd_stop=myRdd2.flatMap(lambda x: x.split())\\\n",
    "                .filter(lambda x : x not in stopwords)\n",
    "for words in myRdd_stop.collect():\n",
    "    print(words, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3f72c",
   "metadata": {},
   "source": [
    "### (4) foreach\n",
    "반환값이 없는 action함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "280cb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb6a3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#커널에서 실행할 때 f함수를 이용해 출력\n",
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333574a2",
   "metadata": {},
   "source": [
    "### (5) pipeline \n",
    "함수를 하나씩 실행하지 않고 연이어 한번에 실행 -> 효율 업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63dc9b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = wordRdd\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print(wordsLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7a77c",
   "metadata": {},
   "source": [
    "### (6) 파일에 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "241f68b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "upper2list=wordRdd.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print (type(upper2list))\n",
    "\n",
    "spark.sparkContext.parallelize(upper2list).saveAsTextFile(\"data/ds_spark_wiki_out_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7a3627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: C826-817A\n",
      "\n",
      " C:\\Users\\SW\\Code\\yhm\\data\\ds_spark_wiki_out_s 디렉터리\n",
      "\n",
      "2021-10-22  오후 12:32    <DIR>          .\n",
      "2021-10-22  오후 12:32    <DIR>          ..\n",
      "2021-10-22  오후 12:32                12 .part-00000.crc\n",
      "2021-10-22  오후 12:32                 8 ._SUCCESS.crc\n",
      "2021-10-22  오후 12:32                29 part-00000\n",
      "2021-10-22  오후 12:32                 0 _SUCCESS\n",
      "               4개 파일                  49 바이트\n",
      "               2개 디렉터리  378,133,835,776 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir data\\ds_spark_wiki_out_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71ed2af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['THIS', 'IS']\", \"['A', 'LINE']\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=spark.sparkContext.textFile('data/ds_spark_wiki_out_s')\n",
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22fb8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS']\n",
      "['IS']\n",
      "['A', 'LINE']\n"
     ]
    }
   ],
   "source": [
    "# 파일로 쓰기 coalesce함수\n",
    "#_rdd.map(lambda x:\"\".join(x)).coalesce(1).saveAsTextFile(\"data/ds_spark_wiki_txt\")\n",
    "!type data\\ds_spark_wiki_txt\\part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1623d3",
   "metadata": {},
   "source": [
    "### (7) group By\n",
    "transformation 함수\n",
    "키- 밸류로 데이터가 쌍을 이룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79e10fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi:<pyspark.resultiterable.ResultIterable object at 0x0000020D05736DC0>\n",
      "Ap:<pyspark.resultiterable.ResultIterable object at 0x0000020D05742D60>\n",
      "아파:<pyspark.resultiterable.ResultIterable object at 0x0000020D05742A90>\n",
      "Or:<pyspark.resultiterable.ResultIterable object at 0x0000020D057429D0>\n",
      "th:<pyspark.resultiterable.ResultIterable object at 0x0000020D05742880>\n",
      "wh:<pyspark.resultiterable.ResultIterable object at 0x0000020D057422B0>\n",
      "Sp:<pyspark.resultiterable.ResultIterable object at 0x0000020D057421F0>\n",
      "im:<pyspark.resultiterable.ResultIterable object at 0x0000020D05742160>\n",
      "\n",
      "\n",
      "Wi: Wikipedia\n",
      "-----\n",
      "Ap: Apache Spark is an open source cluster computing framework.\n",
      "Ap: Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "아파: 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파: 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "Or: Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "th: the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "wh: which has maintained it since.\n",
      "-----\n",
      "Sp: Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "im: implicit data parallelism and fault-tolerance.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    print(\"{}:{}\".format(k,v))\n",
    "print('\\n')\n",
    "\n",
    "#ResultIterable을 해체\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print(\"{}: {}\".format(k, eachValue))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3533429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1)]),\n",
       " ('Busan',\n",
       "  [('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1)])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testList=[(\"Seoul\",1),(\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)\n",
    "\n",
    "#ResultIterable을 리스트로 변환하여 값을 볼 때 mapValues() 함수를 사용.\n",
    "_testRdd.groupBy(lambda x:x[0]).collect()\n",
    "_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x: list(x)).collect()\n",
    "#mapValues(list)라고 써도 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e9dd7",
   "metadata": {},
   "source": [
    "## 3. Pair RDD\n",
    "key-value쌍으로 되어 있는 데이터\n",
    "```\n",
    "구분\t설명\n",
    "groupByKey()\t같은 key를 grouping, partition에서 먼저 reduce하지 않고, 전체로 계산한다.\n",
    "reduceByKey()\t같은 key의 value를 합계, partition에서 먼저 reduce하고, 전체로 계산한다. grouping + aggregation.\n",
    "즉 reduceByKey = groupByKey().reduce()\n",
    "\n",
    "combineByKey()\t키별로 합계, 개수 (key, (sum, count))를 계산. createCombiner, mergeValue, mergeCombiners.\n",
    "aggregateByKey()\treduceByKey()와 유사한 기능을 수행한다. 키별로 합계, 개수, 평균을 계산.\n",
    "mapValues()\tPaired RDD는 key,value가 있기 마련이고, value에 적용하는 함수이다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87884298",
   "metadata": {},
   "source": [
    "### (1) 파티션 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2ba7564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)\n",
    "\n",
    "#파티션수 : 클러스터 노드에 배분된 데이터 수 확인\n",
    "print(_testRdd.getNumPartitions())\n",
    "#파티션수 바꾸기\n",
    "_testRdd=spark.sparkContext.parallelize(_testList,2)\n",
    "print(_testRdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6281f8",
   "metadata": {},
   "source": [
    "### (2) groupByKey, reduceByKey, mapValues\n",
    "\n",
    "groupBy 결과는 ResultIterable 따라서 mapValues를 사용해여 값을 볼 수 있다.\n",
    "\n",
    "reduceByKey는 groupByKey()와 달리 키별로 빈도를 합산하기 때문에 mapValues()가 필요없다.<br>\n",
    "reduce()는 함수를 사용해서 인자를 2개 받아서 1개로 병합하는 기능을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d101923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', [1, 1, 1, 1, 1, 1]), ('key2', [1, 1, 1, 1, 1])]\n",
      "[('key1', 6), ('key2', 5)]\n",
      "[('key1', 2), ('key1', 2), ('key1', 2), ('key2', 2), ('key2', 2), ('key1', 2), ('key2', 2), ('key1', 2), ('key1', 2), ('key2', 2), ('key2', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(_testRdd.groupByKey().mapValues(list).collect())\n",
    "print(_testRdd.reduceByKey(lambda x,y:x+y).collect())\n",
    "print(_testRdd.mapValues(lambda x:x+1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ddba7",
   "metadata": {},
   "source": [
    "- 단어 빈도 예제\n",
    "```\n",
    "1    flatMap()은 RDD를 전체로 flatten해서 공백으로 분리하고, 단어빈도를 계산한다. map()은 요소별로 적용된다.\n",
    "2    요소에 대해 단어빈도 (x,1)를 만듦\n",
    "3    groupByKey()는 key를 묶어준다. 따라서 ResultIterable iterator를 반환한다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab990c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어:AMPLab,, 빈도: 1\n",
      "단어:Apache, 빈도: 6\n",
      "단어:Berkeley's, 빈도: 1\n",
      "단어:California,, 빈도: 1\n",
      "단어:Foundation,, 빈도: 1\n"
     ]
    }
   ],
   "source": [
    "# groupByKey\n",
    "wc = myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x: (x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .sortByKey(True)\n",
    "\n",
    "for e in wc.take(5):\n",
    "    print(f\"단어:{e[0]}, 빈도: {e[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "399e2581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1), ('Apache', 6), ('Spark', 7), ('is', 1), ('an', 2)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x: (x,1))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cccb22",
   "metadata": {},
   "source": [
    "### (3) countByKey\n",
    "key 별로 계산하는 action함수, dict형식으로 결과가 반환됨\n",
    "\n",
    "list반환이 필요한 경우 countByKey().items()하면 리스크로 변환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c51bd4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Wikipedia': 1,\n",
       "             'Apache': 6,\n",
       "             'Spark': 7,\n",
       "             'is': 1,\n",
       "             'an': 2,\n",
       "             'open': 1,\n",
       "             'source': 1,\n",
       "             'cluster': 1,\n",
       "             'computing': 1,\n",
       "             'framework.': 1,\n",
       "             '아파치': 5,\n",
       "             '스파크는': 1,\n",
       "             '오픈': 1,\n",
       "             '소스': 1,\n",
       "             '클러스터': 1,\n",
       "             '컴퓨팅': 1,\n",
       "             '프레임워크이다.': 1,\n",
       "             '스파크': 4,\n",
       "             'Originally': 1,\n",
       "             'developed': 1,\n",
       "             'at': 1,\n",
       "             'the': 3,\n",
       "             'University': 1,\n",
       "             'of': 1,\n",
       "             'California,': 1,\n",
       "             \"Berkeley's\": 1,\n",
       "             'AMPLab,': 1,\n",
       "             'codebase': 1,\n",
       "             'was': 1,\n",
       "             'later': 1,\n",
       "             'donated': 1,\n",
       "             'to': 1,\n",
       "             'Software': 1,\n",
       "             'Foundation,': 1,\n",
       "             'which': 1,\n",
       "             'has': 1,\n",
       "             'maintained': 1,\n",
       "             'it': 1,\n",
       "             'since.': 1,\n",
       "             'provides': 1,\n",
       "             'interface': 1,\n",
       "             'for': 1,\n",
       "             'programming': 1,\n",
       "             'entire': 1,\n",
       "             'clusters': 1,\n",
       "             'with': 1,\n",
       "             'implicit': 1,\n",
       "             'data': 1,\n",
       "             'parallelism': 1,\n",
       "             'and': 1,\n",
       "             'fault-tolerance.': 1})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x :x.split())\\\n",
    "    .map(lambda x: (x,1))\\\n",
    "    .countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1a658",
   "metadata": {},
   "source": [
    "# 문제 S-2 : RDD를 사용하여 단어빈도를 계산하고, 그래프 그리기.\n",
    "텍스트 파일을 읽어서, 단어빈도를 계산하는 프로그램을 작성하세요. 단어빈도를 내림차순으로 출력해서 상위 15개를 출력하세요.\n",
    "\n",
    "data/ds_bigdata_wiki.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad313829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big data\n",
      "활용사례 및 의의[편집]\n",
      "정치 및 사회[편집]\n",
      "2008년 미국 대통령 선거[편집]\n",
      "2008년 미국 대통령 선거에서 버락 오바마 미국 대통령 후보는 다양한 형태의 유권자 데이터베이스를 확보하여 이를 분석, 활용한 '유권자 맞춤형 선거 전략'을 전개했다. 당시 오바마 캠프는 인종, 종교, 나이, 가구형태, 소비수준과 같은 기본 인적 사항으로 유권자를 분류하는 것을 넘어서서 과거 투표 여부, 구독하는 잡지, 마시는 음료 등 유권자 성향까지 전화나 개별 방문을 또는 소셜 미디어를 통해 유권자 정보를 수집하였다. 수집된 데이터는 오바마 캠프 본부로 전송되어 유권자 데이터베이스를 온라인으로 통합관리하는 ‘보트빌더(VoteBuilder.com)’시스템의 도움으로 유권자 성향 분석, 미결정 유권자 선별 , 유권자에 대한 예측을 해나갔다. 이를 바탕으로‘유권자 지도’를 작성한 뒤 ‘유권자 맞춤형 선거 전략’을 전개하는 등 오바마 캠프는 비용 대비 효과적인 선거를 치를 수 있었다.\n"
     ]
    }
   ],
   "source": [
    "myRdd3=spark.sparkContext.textFile(os.path.join('data','ds_bigdata_wiki.txt'))\n",
    "for i in myRdd3.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7dadc39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big/data/활용사례/및/의의[편집]/정치/및/사회[편집]/"
     ]
    }
   ],
   "source": [
    "wc3 = myRdd3.map(lambda x : x.split())\n",
    "for i in wc3.take(3):\n",
    "    for j in i:\n",
    "        print(j, end='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257a66a",
   "metadata": {},
   "source": [
    "불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "482f3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "21 데이터\n",
      "18 데이터를\n",
      "14 빅\n",
      "9 있다.\n",
      "8 수\n",
      "8 데이터의\n",
      "7 미국\n",
      "7 통해\n",
      "6 유권자\n",
      "6 선거\n",
      "6 대한\n",
      "6 빅데이터\n",
      "5 활용한\n",
      "5 소셜\n",
      "5 대한민국\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(['및','이를','등','이','이런','그와','또는','두', '이와', '전', '간'])\n",
    "\n",
    "wc3=myRdd3\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .filter(lambda x: x.lower() not in stopwords)\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(15)\n",
    "\n",
    "print (type(wc3))\n",
    "for i in wc3:\n",
    "    print (i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac14b99",
   "metadata": {},
   "source": [
    "막대그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64ae0dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=list()\n",
    "v=list()\n",
    "for i in wc3:\n",
    "    k.append(i[1])\n",
    "    v.append(i[0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.barh(range(len(v)), v)\n",
    "plt.yticks(range(len(v)),k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68526ba",
   "metadata": {},
   "source": [
    "# 문제 S-3: 성적 합계 및 평균.\n",
    "아래 데이터를 RDD로 만들고, 성적의 합계 및 평균을 계산하세요. 하위 문제별로 RDD를 생성하지 말고, 원본데이터에서 하나의 RDD만을 생성하고, 이를 변형해서 문제를 푸세요.\n",
    "```\n",
    "이름\t과목\t점수\n",
    "김하나\tEnglish\t100\n",
    "김하나\tMath\t80\n",
    "임하나\tEnglish\t70\n",
    "임하나\tMath\t100\n",
    "김갑돌\tEnglish\t82.3\n",
    "김갑돌\tMath\t98.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356139f",
   "metadata": {},
   "source": [
    "문제 3-1: 이름으로 합계를 구해보자. 올바른 출력은 다음과 같다. 이름과 점수로 데이터를 추출하고, 이름별로 (이름을 키로) 합계를 계산한다.\n",
    "```\n",
    "'임하나' 170.0\n",
    "'김하나' 180.0\n",
    "'김갑돌' 180.8\n",
    "```\n",
    "문제 3-2: 과목으로 합계를 계산해 보자. 출력은 다음과 같이 나와야 한다. 과목과 점수로 데이터를 추출하여, 과목별로 (과목을 키로) 합계를 계산한다.\n",
    "```\n",
    "'English' 252.3\n",
    "'Math' 278.5\n",
    "```\n",
    "문제 3-3: 이름으로 합계과 개수를 구해보자. 출력은 다음과 같이 계산된다. 이름과 점수로 데이터를 추출하여, 이름별로 (이름을 키로) 합계와 개수를 계산한다.\n",
    "```\n",
    "'임하나' (170.0, 2)\n",
    "'김하나' (180.0, 2)\n",
    "'김갑돌' (180.8, 2)\n",
    "```\n",
    "문제 3-4: 이름으로 평균을 계산해 보자. 앞서 3-3에서 사용했던 결과를 활용하고, 올바른 출력은 다음과 같다.\n",
    "```\n",
    "'임하나' 85.0\n",
    "'김하나' 90.0\n",
    "'김갑돌' 90.4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7936af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'임하나' 170.0\n",
      "'김하나' 180.0\n",
      "'김갑돌' 180.8\n",
      "\n",
      "\n",
      "' English' 252.3\n",
      "' Math' 278.5\n",
      "\n",
      "\n",
      "'김갑돌' 90.4\n",
      "'김하나' 90.0\n",
      "'임하나' 85.0\n"
     ]
    }
   ],
   "source": [
    "marks=marks = [\"김하나, English, 100\",\n",
    "         \"김하나, Math, 80\",\n",
    "         \"임하나, English, 70\",\n",
    "         \"임하나, Math, 100\",\n",
    "         \"김갑돌, English, 82.3\",\n",
    "         \"김갑돌, Math, 98.5\"\n",
    "        ]\n",
    "\n",
    "marksRdd=spark.sparkContext.parallelize(marks)\n",
    "\n",
    "#문제 1\n",
    "marksSum = marksRdd.map(lambda x : x.split(','))\\\n",
    "                .map(lambda x: (x[0],(float)(x[2])))\\\n",
    "                .reduceByKey(lambda x,y:x+y)\\\n",
    "                .sortByKey(False).collect()\n",
    "for i in marksSum:\n",
    "    print(f\"'{i[0]}' {i[1]}\")\n",
    "print('\\n')\n",
    "# 문제 2\n",
    "marksSum2 = marksRdd.map(lambda x : x.split(','))\\\n",
    "                .map(lambda x: (x[1],(float)(x[2])))\\\n",
    "                .reduceByKey(lambda x,y:x+y)\\\n",
    "                .sortByKey(True).collect()\n",
    "for i in marksSum2:\n",
    "    print(f\"'{i[0]}' {i[1]}\")\n",
    "print('\\n')\n",
    "\n",
    "#평균\n",
    "marksAvg = marksRdd.map(lambda x: x.split(','))\\\n",
    "            .map(lambda x: (x[0],(float)(x[2])))\\\n",
    "            .combineByKey(lambda value: (value,1),\n",
    "                        lambda x,value: (x[0]+value, x[1]+1),\n",
    "                        lambda x,y: (x[0]+y[0], x[1]+y[1]))\\\n",
    "            .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "            .sortByKey(True).collect()\n",
    "\n",
    "for i in marksAvg:\n",
    "    print(f\"'{i[0]}' {i[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e4497",
   "metadata": {},
   "source": [
    "# 문제 S-4: 서울시 지하철호선별 승차인원 평균 구하기.\n",
    "\n",
    "문제 : 정량데이터는 보통 집단화하여 빈도, 평균, 합계 등 서술통계를 계산한다. 서울시 지하철호선별 역별 승하차 인원 정보를 가져와 평균을 구해보자.\n",
    "\n",
    "* 파일 명 CARD_SUBWAY_MONTH_202105.csv를 다운로드 받아서 일부만 테스트용 데이터로 사용한다.\n",
    "* 오픈API 샘플URL http://openapi.seoul.go.kr:8088/(인증키)/xml/CardSubwayStatsNew/1/5/20151101\n",
    "* 답 [('2호선', 10529.0), ('3호선', 9236.0), ('4호선', 5704.0), ('경부선', 19989.6), ('경원선', 1194.75)]\n",
    "```\n",
    "변수명\t타입\t변수설명\t값설명\n",
    "KEY\tString(필수)\t인증키\tOpenAPI 에서 발급된 인증키\n",
    "TYPE\tString(필수)\t요청파일타입\txml : xml, xml파일 : xmlf, 엑셀파일 : xls, json파일 : json\n",
    "SERVICE\tString(필수)\t서비스명\tCardSubwayStatsNew\n",
    "START_INDEX\tINTEGER(필수)\t요청시작위치\t정수 입력 (페이징 시작번호 입니다 : 데이터 행 시작번호)\n",
    "END_INDEX\tINTEGER(필수)\t요청종료위치\t정수 입력 (페이징 끝번호 입니다 : 데이터 행 끝번호)\n",
    "USE_DT\tSTRING(필수)\t사용일자\tYYYYMMDD 형식의 문자열\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d18f9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://openapi.seoul.go.kr:8088/62436e77746b69743237566f4f6f6f/json/CardSubwayStatsNew/1/20/20151101\n",
      "['20151101,1호선,서울역,53097.0,45172.0,20151203', '20151101,1호선,시청,16963.0,16793.0,20151203', '20151101,1호선,종각,25924.0,20608.0,20151203', '20151101,1호선,종로3가,23769.0,19962.0,20151203', '20151101,1호선,종로5가,12945.0,13012.0,20151203', '20151101,1호선,동대문,14825.0,18294.0,20151203', '20151101,1호선,신설동,10187.0,9620.0,20151203', '20151101,1호선,제기동,12830.0,13067.0,20151203', '20151101,1호선,청량리(지하),24023.0,24379.0,20151203', '20151101,1호선,동묘앞,13289.0,13593.0,20151203', '20151101,2호선,시청,12201.0,11375.0,20151203', '20151101,2호선,을지로입구,33374.0,30589.0,20151203', '20151101,2호선,을지로3가,9327.0,8584.0,20151203', '20151101,2호선,을지로4가,4664.0,4554.0,20151203', '20151101,2호선,동대문역사문화공원,13824.0,16949.0,20151203', '20151101,2호선,신당,10588.0,11497.0,20151203', '20151101,2호선,상왕십리,6273.0,6173.0,20151203', '20151101,2호선,왕십리(성동구청),14060.0,12865.0,20151203', '20151101,2호선,한양대,4617.0,5630.0,20151203', '20151101,2호선,뚝섬,9362.0,9446.0,20151203']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import src.mylib\n",
    "import requests\n",
    "\n",
    "#인증키 생성\n",
    "keyPath = os.path.join(os.getcwd(), 'src', 'key.properties')\n",
    "key = src.mylib.getKey(keyPath)\n",
    "KEY = str(key['dataseoul'])\n",
    "\n",
    "#인자\n",
    "TYPE='json'\n",
    "SERVICE='CardSubwayStatsNew'\n",
    "START_INDEX=str(1)\n",
    "END_INDEX=str(20)\n",
    "USE_DT='20151101'\n",
    "\n",
    "#url\n",
    "_url ='http://openapi.seoul.go.kr:8088'\n",
    "url = '/'.join([_url, KEY, TYPE, SERVICE, START_INDEX, END_INDEX, USE_DT])\n",
    "print(url)\n",
    "\n",
    "r=requests.get(url)\n",
    "sub=r.json()\n",
    "# print(sub['CardSubwayStatsNew']['row'])\n",
    "_sub = sub['CardSubwayStatsNew']['row']\n",
    "_s=[]\n",
    "for i in _sub:\n",
    "    u=str(i['USE_DT'])\n",
    "    l=i['LINE_NUM']\n",
    "    s=i['SUB_STA_NM']\n",
    "    r=str(i['RIDE_PASGR_NUM'])\n",
    "    a=str(i[\"ALIGHT_PASGR_NUM\"])\n",
    "    w=str(i['WORK_DT'])\n",
    "    _sum = u+','+l+','+s+','+r+','+a+','+w\n",
    "    _s.append(_sum)\n",
    "#\"사용일자\",\"노선명\",\"역ID\",\"역명\",승차총승객수,하차총승객수\n",
    "#print(_s)\n",
    "_subRdd = spark.sparkContext.parallelize(_s)\n",
    "print(_subRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a47f7674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1호선', 20785.2), ('2호선', 11829.0)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subAvg = _subRdd\\\n",
    "            .map(lambda x : x.split(','))\\\n",
    "            .map(lambda x :(x[1],(float)(x[3])))\\\n",
    "            .combineByKey(\n",
    "                (lambda x: (x, 1)), \n",
    "                (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "                (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1]))\n",
    "            )\\\n",
    "            .map(lambda x: (x[0],x[1][0]/x[1][1]))\n",
    "_subAvg.collect() #왜?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c3d3b650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2호선', 10529.0),\n",
       " ('3호선', 9236.0),\n",
       " ('4호선', 5704.0),\n",
       " ('경부선', 19989.6),\n",
       " ('경원선', 1194.75)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일부를 가져와서 사용한다.\n",
    "_sub=[\"20150101,2호선,0236,영등포구청,6199,6219\",\n",
    "\"20150101,2호선,0237,당산,7982,8946\",\n",
    "\"20150101,2호선,0238,합정,17406,15241\",\n",
    "\"20150101,3호선,0309,지축,515,538\",\n",
    "\"20150101,3호선,0310,구파발,6879,6260\",\n",
    "\"20150101,3호선,0311,연신내,20031,19470\",\n",
    "\"20150101,3호선,0312,불광,9519,11029\",\n",
    "\"20150101,4호선,0425,회현,7465,7574\",\n",
    "\"20150101,4호선,0426,서울역,3943,10823\",\n",
    "\"20150101,경부선,1002,남영,4340,4535\",\n",
    "\"20150101,경부선,1003,용산,28980,27684\",\n",
    "\"20150101,경부선,1004,노량진,23021,23862\",\n",
    "\"20150101,경부선,1005,대방,6360,6476\",\n",
    "\"20150101,경부선,1006,영등포,37247,36102\",\n",
    "\"20150101,경원선,1008,이촌,1940,1507\",\n",
    "\"20150101,경원선,1009,서빙고,911,1000\",\n",
    "\"20150101,경원선,1010,한남,1885,1863\",\n",
    "\"20150101,경원선,1011,옥수,43,37\"]\n",
    "\n",
    "subRdd = spark.sparkContext.parallelize(_sub)\n",
    "subAvg = subRdd.map(lambda x: x.split(','))\\\n",
    "            .map(lambda x :(x[1],(int)(x[4])))\\\n",
    "            .combineByKey(\n",
    "                (lambda x: (x, 1)), \n",
    "                (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "                (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1]))\n",
    "            )\\\n",
    "            .map(lambda x: (x[0],x[1][0]/x[1][1]))\n",
    "\n",
    "subAvg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72191162",
   "metadata": {},
   "source": [
    "# # 문제 S-4: RDD를 사용하여 word vector를 생성하기.\n",
    "문제 : data/ds_spark_wiki.txt파일을 이용하여 word vector(단어빈도)를 계산한다.\n",
    "* 단어개수\n",
    "* 전체 단어 목록\n",
    "* 단어 빈도 집계\n",
    "\n",
    "```\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782a86b",
   "metadata": {},
   "source": [
    "### (1)reduceBykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2cf9d3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 2),\n",
       " ('and', 1),\n",
       " ('apache', 6),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRdd = spark.sparkContext.textFile(os.path.join('data','ds_spark_wiki.txt'))\n",
    "\n",
    "\n",
    "wordscount = wordsRdd\\\n",
    "            .flatMap(lambda x: x.split())\\\n",
    "            .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "            .reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "wordscount.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca7a4d",
   "metadata": {},
   "source": [
    "### (2) groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ff97d5cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 2),\n",
       " ('and', 1),\n",
       " ('apache', 6),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordscount.groupByKey().mapValues(sum).sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "64b87546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 1),\n",
       " ('and', 1),\n",
       " ('apache', 1),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordscount.groupByKey().map(lambda x: (x[0],len(x[1]))).sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23e41a",
   "metadata": {},
   "source": [
    "### (3) 파일로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0ec2b643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_rdd_wordCount.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile src/ds_rdd_wordCount.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "def doIt():\n",
    "    wikiRdd=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "    wc=wikiRdd\\\n",
    "        .flatMap(lambda x: x.split())\\\n",
    "        .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .sortByKey()\\\n",
    "        .collect()\n",
    "    for w in wc:\n",
    "        print (w)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1b6e5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: C826-817A\n",
      "\n",
      " C:\\Users\\SW\\Code\\yhm\\src 디렉터리\n",
      "\n",
      "2021-10-22  오후 05:08               683 ds_rdd_wordCount.py\n",
      "               1개 파일                 683 바이트\n",
      "               0개 디렉터리  378,093,867,008 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir src\\ds_rdd_wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "65d98665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amplab', 1)\n",
      "('an', 2)\n",
      "('and', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/spark/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/10/22 17:08:55 INFO SparkContext: Running Spark version 3.1.2\n",
      "21/10/22 17:08:55 INFO ResourceUtils: ==============================================================\n",
      "21/10/22 17:08:55 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/10/22 17:08:55 INFO ResourceUtils: ==============================================================\n",
      "21/10/22 17:08:55 INFO SparkContext: Submitted application: myApp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('apache', 6)\n",
      "('at', 1)\n",
      "(\"berkeley's\", 1)\n",
      "('california', 1)\n",
      "('cluster', 1)\n",
      "('clusters', 1)\n",
      "('codebase', 1)\n",
      "('computing', 1)\n",
      "('data', 1)\n",
      "('developed', 1)\n",
      "('donated', 1)\n",
      "('entire', 1)\n",
      "('fault-tolerance', 1)\n",
      "('for', 1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/22 17:08:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/10/22 17:08:56 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/10/22 17:08:56 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/10/22 17:08:56 INFO SecurityManager: Changing view acls to: SW중심대학사업단,SW?????л?¾÷\n",
      "21/10/22 17:08:56 INFO SecurityManager: Changing modify acls to: SW중심대학사업단,SW?????л?¾÷\n",
      "21/10/22 17:08:56 INFO SecurityManager: Changing view acls groups to: \n",
      "21/10/22 17:08:56 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/10/22 17:08:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SW중심대학사업단, SW?????л?¾÷); groups with view permissions: Set(); users  with modify permissions: Set(SW중심대학사업단, SW?????л?¾÷); groups with modify permissions: Set()\n",
      "21/10/22 17:08:57 INFO Utils: Successfully started service 'sparkDriver' on port 52004.\n",
      "21/10/22 17:08:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/10/22 17:08:57 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/10/22 17:08:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/10/22 17:08:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/10/22 17:08:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/10/22 17:08:57 INFO DiskBlockManager: Created local directory at C:\\Users\\SW\\AppData\\Local\\Temp\\blockmgr-c35c7c30-c669-44fc-af27-f4ed1b223440\n",
      "21/10/22 17:08:57 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "21/10/22 17:08:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/10/22 17:08:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/22 17:08:57 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "21/10/22 17:08:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-PQ8LIGD:4041\n",
      "21/10/22 17:08:58 INFO Executor: Starting executor ID driver on host DESKTOP-PQ8LIGD\n",
      "21/10/22 17:08:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52019.\n",
      "21/10/22 17:08:58 INFO NettyBlockTransferService: Server created on DESKTOP-PQ8LIGD:52019\n",
      "21/10/22 17:08:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/10/22 17:08:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-PQ8LIGD, 52019, None)\n",
      "21/10/22 17:08:58 INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-PQ8LIGD:52019 with 434.4 MiB RAM, BlockManagerId(driver, DESKTOP-PQ8LIGD, 52019, None)\n",
      "21/10/22 17:08:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-PQ8LIGD, 52019, None)\n",
      "21/10/22 17:08:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-PQ8LIGD, 52019, None)\n",
      "21/10/22 17:08:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/SW/Code/yhm/spark-warehouse/').\n",
      "21/10/22 17:08:59 INFO SharedState: Warehouse path is 'file:/C:/Users/SW/Code/yhm/spark-warehouse/'.\n",
      "21/10/22 17:09:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 176.1 KiB, free 434.2 MiB)\n",
      "21/10/22 17:09:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)\n",
      "21/10/22 17:09:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-PQ8LIGD:52019 (size: 27.2 KiB, free: 434.4 MiB)\n",
      "21/10/22 17:09:01 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "21/10/22 17:09:02 INFO FileInputFormat: Total input files to process : 1\n",
      "21/10/22 17:09:02 INFO SparkContext: Starting job: collect at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9\n",
      "21/10/22 17:09:02 INFO DAGScheduler: Registering RDD 3 (reduceByKey at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9) as input to shuffle 0\n",
      "21/10/22 17:09:02 INFO DAGScheduler: Got job 0 (collect at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9) with 1 output partitions\n",
      "21/10/22 17:09:02 INFO DAGScheduler: Final stage: ResultStage 1 (collect at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9)\n",
      "21/10/22 17:09:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "21/10/22 17:09:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "21/10/22 17:09:02 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9), which has no missing parents\n",
      "21/10/22 17:09:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)\n",
      "21/10/22 17:09:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 434.2 MiB)\n",
      "21/10/22 17:09:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-PQ8LIGD:52019 (size: 7.2 KiB, free: 434.4 MiB)\n",
      "21/10/22 17:09:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/22 17:09:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/22 17:09:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/10/22 17:09:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-PQ8LIGD, executor driver, partition 0, PROCESS_LOCAL, 4507 bytes) taskResourceAssignments Map()\n",
      "21/10/22 17:09:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/10/22 17:09:02 INFO HadoopRDD: Input split: file:/C:/Users/SW/Code/yhm/data/ds_spark_wiki.txt:0+583\n",
      "21/10/22 17:09:04 INFO PythonRunner: Times: total = 1611, boot = 1591, init = 19, finish = 1\n",
      "21/10/22 17:09:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1614 bytes result sent to driver\n",
      "21/10/22 17:09:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2158 ms on DESKTOP-PQ8LIGD (executor driver) (1/1)\n",
      "21/10/22 17:09:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/10/22 17:09:04 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52020\n",
      "21/10/22 17:09:04 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9) finished in 2.302 s\n",
      "21/10/22 17:09:04 INFO DAGScheduler: looking for newly runnable stages\n",
      "21/10/22 17:09:04 INFO DAGScheduler: running: Set()\n",
      "21/10/22 17:09:04 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "21/10/22 17:09:04 INFO DAGScheduler: failed: Set()\n",
      "21/10/22 17:09:04 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9), which has no missing parents\n",
      "21/10/22 17:09:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.3 KiB, free 434.2 MiB)\n",
      "21/10/22 17:09:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.2 MiB)\n",
      "21/10/22 17:09:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-PQ8LIGD:52019 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "21/10/22 17:09:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/22 17:09:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at collect at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/22 17:09:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "21/10/22 17:09:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-PQ8LIGD, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "21/10/22 17:09:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "21/10/22 17:09:04 INFO ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\n",
      "21/10/22 17:09:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "21/10/22 17:09:06 INFO PythonRunner: Times: total = 1593, boot = 1571, init = 21, finish = 1\n",
      "21/10/22 17:09:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2463 bytes result sent to driver\n",
      "21/10/22 17:09:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1697 ms on DESKTOP-PQ8LIGD (executor driver) (1/1)\n",
      "21/10/22 17:09:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/10/22 17:09:06 INFO DAGScheduler: ResultStage 1 (collect at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9) finished in 1.716 s\n",
      "21/10/22 17:09:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/10/22 17:09:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "21/10/22 17:09:06 INFO DAGScheduler: Job 0 finished: collect at C:/Users/SW/Code/yhm/src/ds_rdd_wordCount.py:9, took 4.110481 s\n",
      "21/10/22 17:09:06 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-PQ8LIGD:4041\n",
      "21/10/22 17:09:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/10/22 17:09:06 INFO MemoryStore: MemoryStore cleared\n",
      "21/10/22 17:09:06 INFO BlockManager: BlockManager stopped\n",
      "21/10/22 17:09:06 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/10/22 17:09:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/10/22 17:09:06 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/10/22 17:09:07 INFO ShutdownHookManager: Shutdown hook called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/22 17:09:07 INFO ShutdownHookManager: Deleting directory C:\\Users\\SW\\AppData\\Local\\Temp\\spark-a365b1af-ef06-4c6d-bfd2-20e0b8ca4856\\pyspark-153fe23d-ef07-4377-ad46-1992f56283d1\n",
      "21/10/22 17:09:07 INFO ShutdownHookManager: Deleting directory C:\\Users\\SW\\AppData\\Local\\Temp\\spark-c0673c76-b945-43db-a5c9-ab20f3f90421\n",
      "21/10/22 17:09:07 INFO ShutdownHookManager: Deleting directory C:\\Users\\SW\\AppData\\Local\\Temp\\spark-a365b1af-ef06-4c6d-bfd2-20e0b8ca4856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('foundation', 1)\n",
      "('framework', 1)\n",
      "('has', 1)\n",
      "('implicit', 1)\n",
      "('interface', 1)\n",
      "('is', 1)\n",
      "('it', 1)\n",
      "('later', 1)\n",
      "('maintained', 1)\n",
      "('of', 1)\n",
      "('open', 1)\n",
      "('originally', 1)\n",
      "('parallelism', 1)\n",
      "('programming', 1)\n",
      "('provides', 1)\n",
      "('since', 1)\n",
      "('software', 1)\n",
      "('source', 1)\n",
      "('spark', 7)\n",
      "('the', 3)\n",
      "('to', 1)\n",
      "('university', 1)\n",
      "('was', 1)\n",
      "('which', 1)\n",
      "('wikipedia', 1)\n",
      "('with', 1)\n",
      "('소스', 1)\n",
      "('스파크', 4)\n",
      "('스파크는', 1)\n",
      "('아파치', 5)\n",
      "('오픈', 1)\n",
      "('컴퓨팅', 1)\n",
      "('클러스터', 1)\n",
      "('프레임워크이다', 1)\n"
     ]
    }
   ],
   "source": [
    "!spark-submit src\\ds_rdd_wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e18c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
